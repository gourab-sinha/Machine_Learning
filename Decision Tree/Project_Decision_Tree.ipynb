{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project Decision Tree.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZhE3+lseFaSQPpyhGeeKJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gourab-sinha/Machine_Learning/blob/master/Decision%20Tree/Project_Decision_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGJgkzAOXh-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Packages\n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrZbNk3doDod",
        "colab_type": "text"
      },
      "source": [
        "# Iris Dataset \n",
        "#### Iris Dataset has high impurity that means our entropy for a node can go more than 1 which is fine and as it not the binary classifier and only four feature present so some cases my Decision tree may not predict more accurate classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3TKbEmj0AJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Decision Tree Class\n",
        "class DecisionTree:\n",
        "  # Types of classes present in iris dataset\n",
        "  classes = ['setosa', 'versicolor', 'virginica']\n",
        "  def __get_class(self,store_detail,X,features):\n",
        "    # Convert Numpy array to pandas DataFrame ease of access.\n",
        "    X = pd.DataFrame([X],columns=features)\n",
        "\n",
        "    # Dynamic list to get the threshold values depending on conditions\n",
        "    store_detail_new = store_detail\n",
        "\n",
        "    # If type is not dict that means we reach to leaf node\n",
        "    while type(store_detail_new[0])==dict:\n",
        "\n",
        "      # Get the threshold value from the dictionary present at 0th index of store_detail_new\n",
        "      feature_threshold = store_detail_new[0]\n",
        "\n",
        "      # Access key val pair \n",
        "      for feature,threshold in feature_threshold.items():\n",
        "\n",
        "        # if thresold value is greater then data point value then go leftsubtree otherwise rightsubtree or if only one direction exists\n",
        "        if X[feature].iloc[0]<=threshold or len(store_detail_new)<=2:\n",
        "          store_detail_new = store_detail_new[1]\n",
        "        else:\n",
        "          store_detail_new = store_detail_new[2]\n",
        "        break\n",
        "    \n",
        "    return store_detail_new[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def predict_class(self,store_detail,X,features):\n",
        "\n",
        "    # Store prediction\n",
        "    classified_result = []\n",
        "\n",
        "    # Iterate over each data point\n",
        "    for i in X:\n",
        "      class_type = self.__get_class(store_detail,i,features)\n",
        "      classified_result.append(class_type)\n",
        "\n",
        "    return classified_result\n",
        "\n",
        "  # Information gain\n",
        "  def __information_gain(self,X,feature):\n",
        "\n",
        "    # Continous values\n",
        "    X_new = X[feature].values\n",
        "    X_new.sort()\n",
        "    total = X.shape[0]\n",
        "    class_with_count = dict(X['target'].value_counts().items())\n",
        "\n",
        "    # Node Entropy\n",
        "    level_entropy = 0\n",
        "    for key,val in class_with_count.items():\n",
        "      level_entropy -= (val/total)*math.log2(val/total)\n",
        "    \n",
        "                      \n",
        "    # Initialize required parameters\n",
        "    max_info_gain = 0\n",
        "    level_threshold = 0\n",
        "    max_gain_ratio = 0\n",
        "\n",
        "    for i in range(1,len(X_new)):\n",
        "      # Define threshold \n",
        "      threshold = (X_new[i-1]+X_new[i])/2\n",
        "      \n",
        "      # Data points below or equal to threshold\n",
        "      X_below_threshold = X[X[feature]<threshold]\n",
        "\n",
        "      # Data points above threshold \n",
        "      X_above_threshold = X[X[feature]>=threshold]\n",
        "\n",
        "      # Data points below and above\n",
        "      total_below = X_below_threshold.shape[0]\n",
        "      total_above = X_above_threshold.shape[0]\n",
        "\n",
        "      info_required_below = 0\n",
        "      if total_below>0:\n",
        "        # Target values for below threshold\n",
        "        Y_below = X_below_threshold['target']\n",
        "\n",
        "        class_with_count = dict(Y_below.value_counts().items())\n",
        "\n",
        "        info_required_below = 0\n",
        "        # Information required below threshold\n",
        "        for key,val in class_with_count.items():\n",
        "          info_required_below += (val/total_below)*math.log2(val/total_below)\n",
        "        \n",
        "        info_required_below = -info_required_below\n",
        "      \n",
        "      info_required_above = 0\n",
        "      if total_above>0:\n",
        "        # Target values for above threshold\n",
        "        Y_above = X_above_threshold['target']\n",
        "\n",
        "        class_with_count = dict(Y_above.value_counts().items())\n",
        "\n",
        "        for key,val in class_with_count.items():\n",
        "          info_required_above+= (val/total_above)*math.log2(val/total_above)\n",
        "\n",
        "        info_required_above = -info_required_above\n",
        "\n",
        "      \n",
        "      \n",
        "      # Information required\n",
        "      info_require = (total_below/total)*info_required_below + (total_above/total)*info_required_above\n",
        "\n",
        "      # # Current gain\n",
        "      current_info_gain = level_entropy - info_require\n",
        "\n",
        "      # Split Information\n",
        "      current_split_info = 0 \n",
        "      if total_below>0:\n",
        "        current_split_info += (total_below/total)*math.log2(total_below/total)\n",
        "      if total_above>0:\n",
        "        current_split_info += (total_above/total)*math.log2(total_above/total)\n",
        "      \n",
        "      current_split_info = -current_split_info\n",
        "\n",
        "      # Current Gain Ratio \n",
        "      current_gain_ratio = 0\n",
        "      if current_split_info>0:\n",
        "        current_gain_ratio = current_info_gain/current_split_info\n",
        "      # Check with previous Max Gain Ratio\n",
        "      if max_gain_ratio <= current_gain_ratio :\n",
        "        level_threshold = threshold\n",
        "        max_info_gain  = current_info_gain\n",
        "        max_gain_ratio = current_gain_ratio\n",
        "\n",
        "    \n",
        "    return max_info_gain, max_gain_ratio, level_entropy, level_threshold\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Decision Tree \n",
        "  def decision_tree(self,X, features,level,targets):\n",
        "    # Maintain details of threshold and selected feature to split upon\n",
        "    store_info = []  \n",
        "\n",
        "    types = dict(X[targets].value_counts().items())\n",
        "    # print(len(types),len(features))\n",
        "    if len(features)==0 or len(types)==1:\n",
        "      print(\"Level \",level)\n",
        "      for key,val in types.items():\n",
        "        print(\"Count of \",self.classes[key],val)\n",
        "      \n",
        "      entropy = 0\n",
        "      total = X.shape[0]\n",
        "      majority = 0\n",
        "      result_class = \"\"\n",
        "      if total!=0:\n",
        "        for key,val in types.items():\n",
        "          entropy -= (val/total)*math.log2(val/total)\n",
        "          if val>majority:\n",
        "            majority = val\n",
        "            result_class = key\n",
        "      \n",
        "      print(\"Current Entropy is\", entropy)\n",
        "      print(\"Reached leaf Node\")\n",
        "      return [result_class]\n",
        "    \n",
        "    # Parameters initialize with default values\n",
        "    selected_feature = \"\"\n",
        "    max_info_gain = 0\n",
        "    level_gain_ratio = 0\n",
        "    level_entropy = 0\n",
        "    level_threshold = 0\n",
        "\n",
        "    # Iterate over each feature and pick best one \n",
        "    for feature in features:\n",
        "      gain, gain_ratio, entropy, threshold = self.__information_gain(X,feature)\n",
        "      if gain_ratio>=level_gain_ratio:\n",
        "        selected_feature = feature\n",
        "        max_gain = gain\n",
        "        level_gain_ratio = gain_ratio \n",
        "        level_entropy = entropy\n",
        "        level_threshold = threshold  \n",
        "\n",
        "    # Information frequency with target value. \n",
        "    count_with_classes = dict(X[targets].value_counts().items())\n",
        "    print(\"Level \", level)\n",
        "    for key,val in count_with_classes.items():\n",
        "      print(\"Count of \"+self.classes[key],val)\n",
        "    print(\"Current Entropy is \",level_entropy)\n",
        "    print(\"Splitting on feature \"+str(selected_feature)+ \" with gain ratio\",level_gain_ratio)\n",
        "\n",
        "    # Copy of feature to avoid reference issue\n",
        "    # Removing may cause removing from actual features which may cause problem to other recursion call\n",
        "    new_features = features.copy()\n",
        "    new_features.remove(selected_feature)\n",
        "\n",
        "    store_info.append({selected_feature:level_threshold})\n",
        "    X1 = X[X[selected_feature]<=level_threshold].copy()\n",
        "    X2 = X[X[selected_feature]>level_threshold].copy()\n",
        "\n",
        "    # If there exists at least one point which goes in leftsubtree then store the returned result\n",
        "    if(X1.shape[0]>0):\n",
        "      print()\n",
        "      store_info.append(self.decision_tree(X1,new_features,level+1,targets))\n",
        "\n",
        "    # If there exists at least one point which goes in rightsubtree then store the returned result\n",
        "    if X2.shape[0]>0:\n",
        "      print()\n",
        "      store_info.append(self.decision_tree(X2,new_features,level+1,targets))\n",
        "    return store_info "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMVPPT8as5bM",
        "colab_type": "code",
        "outputId": "42027f99-6baa-4aec-9442-7fea5a075461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load data\n",
        "iris = datasets.load_iris()\n",
        "X = pd.DataFrame(iris.data,columns = iris.feature_names)\n",
        "\n",
        "# Added target for future use\n",
        "X['target'] = iris.target\n",
        "features = iris.feature_names\n",
        "\n",
        "# Target values\n",
        "Y = pd.DataFrame(iris.target,columns=['Target'])\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.20)\n",
        "\n",
        "# Decision Tree object and function call\n",
        "dc_tree = DecisionTree()\n",
        "store_detail = dc_tree.decision_tree(X_train,features,0,\"target\")"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Level  0\n",
            "Count of virginica 42\n",
            "Count of versicolor 40\n",
            "Count of setosa 38\n",
            "Current Entropy is  1.583759753416363\n",
            "Splitting on feature petal length (cm) with gain ratio 0.20040451915865418\n",
            "\n",
            "Level  1\n",
            "Count of virginica 42\n",
            "Count of versicolor 40\n",
            "Count of setosa 37\n",
            "Current Entropy is  1.5830175105308546\n",
            "Splitting on feature sepal width (cm) with gain ratio 0.2039216215090233\n",
            "\n",
            "Level  2\n",
            "Count of setosa 2\n",
            "Count of virginica 1\n",
            "Count of versicolor 1\n",
            "Current Entropy is  1.5\n",
            "Splitting on feature sepal length (cm) with gain ratio 0.5\n",
            "\n",
            "Level  3\n",
            "Count of virginica 1\n",
            "Count of setosa 1\n",
            "Current Entropy is  1.0\n",
            "Splitting on feature petal width (cm) with gain ratio 0\n",
            "\n",
            "Level  4\n",
            "Count of  virginica 1\n",
            "Count of  setosa 1\n",
            "Current Entropy is 1.0\n",
            "Reached leaf Node\n",
            "\n",
            "Level  3\n",
            "Count of versicolor 1\n",
            "Count of setosa 1\n",
            "Current Entropy is  1.0\n",
            "Splitting on feature petal width (cm) with gain ratio 0\n",
            "\n",
            "Level  4\n",
            "Count of  versicolor 1\n",
            "Count of  setosa 1\n",
            "Current Entropy is 1.0\n",
            "Reached leaf Node\n",
            "\n",
            "Level  2\n",
            "Count of virginica 41\n",
            "Count of versicolor 39\n",
            "Count of setosa 35\n",
            "Current Entropy is  1.5818794047005236\n",
            "Splitting on feature petal width (cm) with gain ratio 0.18104382956762125\n",
            "\n",
            "Level  3\n",
            "Count of virginica 40\n",
            "Count of versicolor 39\n",
            "Count of setosa 35\n",
            "Current Entropy is  1.5826030323735485\n",
            "Splitting on feature sepal length (cm) with gain ratio 0.11032429676590957\n",
            "\n",
            "Level  4\n",
            "Count of  virginica 40\n",
            "Count of  versicolor 39\n",
            "Count of  setosa 35\n",
            "Current Entropy is 1.5826030323735485\n",
            "Reached leaf Node\n",
            "\n",
            "Level  3\n",
            "Count of  virginica 1\n",
            "Current Entropy is 0.0\n",
            "Reached leaf Node\n",
            "\n",
            "Level  1\n",
            "Count of  setosa 1\n",
            "Current Entropy is 0.0\n",
            "Reached leaf Node\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymLnGtZtP9AB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Features\n",
        "iris = datasets.load_iris()\n",
        "features = iris.feature_names\n",
        "\n",
        "# Drop target column as predict function doesn't require\n",
        "X_test.drop(['target'],axis = 1, inplace=True)\n",
        "\n",
        "# Predict class\n",
        "Predicted_class = dc_tree.predict_class(store_detail,X_test.values,features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypfV6oskd6U9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_test = Y_test.values\n",
        "for i in range(len(Y_test)):\n",
        "  print(Predicted_class[i],Y_test[i])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}