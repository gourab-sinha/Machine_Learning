{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gourab-sinha/Machine_Learning/blob/master/Naive%20Bayes%20Classifier/Project/Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "roQtWaIxtmBg",
        "outputId": "041276fb-51ea-4e24-d150-1e8e56b0b321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Load Packages\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "nltk.download(\"stopwords\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8ucyVY3xzxUL"
      },
      "source": [
        "# Fetch words from 20 different categories\n",
        "#### Categories are labelled with integer values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lFE0YjDht_W9",
        "colab": {}
      },
      "source": [
        "class TextClassification:\n",
        "\n",
        "    # Categories with integer labels\n",
        "    categories = {1:\"alt_atheism\",2:\"comp_graphics\",3:\"comp_os_ms_windows_misc\",4:\"comp_sys_ibm_pc_hardware\",\n",
        "              5:\"comp_sys_mac_hardware\",6:\"comp_windows_x\",7:\"misc_forsale\",8:\"rec_autos\",\n",
        "              9:\"rec_motorcycles\",10:\"rec_sport_baseball\",11:\"rec_sport_hockey\",12:\"sci_crypt\",\n",
        "              13:\"sci_electronics\",14:\"sci_med\",15:\"sci_space\",16:\"soc_religion_christian\",\n",
        "              17:\"talk_politics_guns\",18:\"talk_politics_mideast\",19:\"talk_politics_misc\",\n",
        "              20:\"talk_religion_misc\"}\n",
        "\n",
        "    # Stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    \n",
        "    # Function to get dictionary in the form of list(key-value pair for each element in list)\n",
        "    def get_word_dictionary(self):\n",
        "\n",
        "        # Store words with occurrence \n",
        "        dictionary = {}\n",
        "\n",
        "        # Base Path \n",
        "        base_path = \"/Users/gourabsinha/Desktop/Machine Learning/Naive Bayes Classifier/\"\n",
        "\n",
        "        # Iterate for each category\n",
        "        for i in range(1,21):\n",
        "            # File location\n",
        "            path = base_path+str(self.categories[i])+\".txt\"\n",
        "            with open(path, encoding=\"utf8\", errors='ignore') as file_obj:\n",
        "                file_data =  file_obj.readlines()\n",
        "\n",
        "            # Iterate over each line and pick words except stop words\n",
        "            for i in range(len(file_data)):\n",
        "                for word in file_data[i].split():\n",
        "                    if word not in self.stop_words:\n",
        "                        dictionary[word.lower()] = dictionary.get(word.lower(),0)+1\n",
        "\n",
        "        # Special and digits\n",
        "        ignore_words_with_specialcharacters_digits = set(punctuation)\n",
        "        for i in range(0,10):\n",
        "            ignore_words_with_specialcharacters_digits.add(str(i))\n",
        "\n",
        "        # Remove all keys with special characters and digits\n",
        "        for key in dictionary.copy():\n",
        "            for char in ignore_words_with_specialcharacters_digits:\n",
        "                if char in key:\n",
        "                    dictionary.pop(key)\n",
        "                    break\n",
        "\n",
        "        # Sort based on occurrence\n",
        "        sorted_dictionary = dict(sorted(dictionary.items(), key=operator.itemgetter(1),reverse=True))\n",
        "        words = [key for key,val in sorted_dictionary.items()]\n",
        "        value = [val for key,val in sorted_dictionary.items()]\n",
        "\n",
        "        # Pick top 2000 most appeared words with their count\n",
        "        top_100_words = []\n",
        "        i = 0\n",
        "        for key,val in sorted_dictionary.items():\n",
        "            top_100_words.append([key,val])\n",
        "            i+=1\n",
        "            if i==100:\n",
        "                break\n",
        "\n",
        "        return top_100_words\n",
        "    \n",
        "    def get_words_one_datapoint(self,file_data):\n",
        "        dictionary = {}\n",
        "        \n",
        "        # Iterate over each line and pick words except stop words\n",
        "        for i in range(len(file_data)):\n",
        "            for word in file_data[i].split():\n",
        "                if word not in self.stop_words:\n",
        "                    dictionary[word.lower()] = dictionary.get(word.lower(),0)+1\n",
        "        \n",
        "        # Special and digits\n",
        "        ignore_words_with_specialcharacters_digits = set(punctuation)\n",
        "        for i in range(0,10):\n",
        "            ignore_words_with_specialcharacters_digits.add(str(i))\n",
        "\n",
        "        # Remove all keys with special characters and digits\n",
        "        for key in dictionary.copy():\n",
        "            for char in ignore_words_with_specialcharacters_digits:\n",
        "                if char in key:\n",
        "                    dictionary.pop(key)\n",
        "                    break\n",
        "\n",
        "        # Sort based on occurrence\n",
        "        sorted_dictionary = dict(sorted(dictionary.items(), key=operator.itemgetter(1),reverse=True))\n",
        "        \n",
        "        return sorted_dictionary\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "    def build_dataset(self,top_words):\n",
        "        base_path = \"/Users/gourabsinha/Desktop/Machine Learning/Naive Bayes Classifier/20_newsgroups/\"\n",
        "        dataset = []\n",
        "        words = {}\n",
        "        for i in top_words:\n",
        "            words[i[0]] = 0\n",
        "        # Iterate for each category\n",
        "        for i in range(1,21):\n",
        "            path = base_path + str(self.categories[i])+\"/\"\n",
        "\n",
        "            # Iterate over each file of the ith category(Only 70% of the dataset)\n",
        "            for j in range(1,701):\n",
        "\n",
        "                # File path\n",
        "                file_path = path+str(j)+\".txt\"\n",
        "\n",
        "                # Copy the words\n",
        "                data_point = words.copy()\n",
        "                with open(file_path, encoding=\"utf8\", errors='ignore') as file_obj:\n",
        "                    file_data =  file_obj.readlines()\n",
        "                \n",
        "                # Get the words for the indiviual file\n",
        "                data_point_dictionary = self.get_words_one_datapoint(file_data)\n",
        "\n",
        "                # Check whether present or not \n",
        "                for word in data_point.keys():\n",
        "                    if word in data_point_dictionary.keys():\n",
        "                        data_point[word] = data_point_dictionary[word]\n",
        "                \n",
        "                # Insert the category to the datapoint dictionary\n",
        "                data_point['Target'] = self.categories[i]\n",
        "                dataset.append(data_point)\n",
        "        \n",
        "        # Return as Pandas DataFrame\n",
        "        columns = [i[0] for i in top_words]\n",
        "        columns.append('Target')\n",
        "        df = pd.DataFrame(dataset,columns=columns)\n",
        "        return df\n",
        "      \n",
        "    def train_model(self,data_frame):\n",
        "\n",
        "        # Return dictionary with word counts\n",
        "        pass\n",
        "      \n",
        "                \n",
        "\n",
        "        \n",
        "      \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H4SyFPdm73eW",
        "colab": {}
      },
      "source": [
        "clf = TextClassification()\n",
        "top_words = clf.get_word_dictionary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGo-xHLCYQ1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = clf.build_dataset(top_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "278-eOv6YQ1K",
        "colab_type": "code",
        "outputId": "2fe9b0c5-b141-4dae-ece8-1686e8517200",
        "colab": {}
      },
      "source": [
        "print(*dataset.iloc[6400])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 rec_sport_baseball\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}